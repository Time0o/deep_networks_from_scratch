\documentclass{article}

\usepackage{float}
\usepackage{svg}

\begin{document}

\title{DD2424 - Assignment 1}
\author{Timo Nicolai}

\maketitle

\noindent
I verified my implementation of the analytical gradient by computing the relative
error with respect to the simple gradient calculated via forward difference.
I reimplemented this numerical gradient calculation in Python.

\begin{figure}[H]
  \centering
    \tiny
    \begin{tabular}{|c|c|c|c|}
    \multicolumn{4}{c}{$\nabla W$}                                  \\
    \hline
    data dimensions & batch size & $\lambda$ & $\epsilon_{rel,max}$ \\
    \hline
    20              & 1          & 0         & 1.31e-7              \\
    \hline
    20              & 1          & 0.5       & 4.69e-5              \\
    \hline
    20              & 20         & 0         & 5.24e-5              \\
    \hline
    20              & 20         & 0.5       & 5.14e-3              \\
    \hline

    \multicolumn{4}{c}{}                                            \\

    \multicolumn{4}{c}{$\nabla b$}                                  \\
    \hline
    data dimensions & batch size & $\lambda$ & $\epsilon_{rel,max}$ \\
    \hline
    20              & 1          & 0         & 2.25e-7              \\
    \hline
    20              & 1          & 0.5       & 2.25e-7              \\
    \hline
    20              & 20         & 0         & 2.38e-5              \\
    \hline
    20              & 20         & 0.5       & 2.38e-5              \\
    \hline
    \end{tabular}
  \caption{Maximum relative gradient error between analytical and numerical
           gradient. Maximum taken over all elements of a gradient matrix
           resulting from the same random weight matrix initialization. Numerical
           gradient obtained via simple forward difference with $h = 1e-6$.}
  \label{fig:gradient}
\end{figure}

\noindent
Figure~\ref{fig:gradient} shows the maximum relative error over all gradient
elements for different batch sizes and regularization parameters. All of these
are reasonably small so I assume my implementation is correct.

\bigskip

\begin{figure}
  \centering
    \includesvg[width=0.7\textwidth]{../figures/learning_curves_crossentropy.svg}
  \caption{Learning curves for different learning rates and regularization
           parameters.}
  \label{fig:learning_curves}
\end{figure}

\noindent
Figure~\ref{fig:learning_curves} shows learning curves for the requested
hyperparameter settings. The first set of curves clearly shows that if the
learning rate is chosen too large, loss (and thus accuracy) may not converge
smoothly or at all. While we initially still observe a downwards trend in the
loss for $\eta = 0.1$, the learning curve fluctuates strongly and it seems
unlikely that training will reach a good local minimum.

Increasing $\lambda$ decreases the distance between training and validation set
error. Intuitively, regularization makes it harder for the training algorithm
to overfit on the training set. Another consequence of this is that the
training error does not converge as quickly. Ideally the first of these effects
should dominate the other so that regularization results in an overall decrease
in validation error. Here however we observe that higher regularization leads
both to higher training and validation errors, possibly because the model is
not very complex and the training set is not that large.

\begin{figure}[H]
  \centering
    \includesvg[width=\textwidth]{../figures/weights_crossentropy.svg}
  \caption{Image representations of weight matrices of different networks
           trained for 40 epochs each.}
  \label{fig:weights}
\end{figure}

\noindent
Figure~\ref{fig:weights} shows the weights learned by the different networks.
Observe how increasing the amount of regularization ``smoothes out'' the weights.

\begin{figure}[H]
  \centering
    \includesvg[width=.7\textwidth]{../figures/confusion_matrices_crossentropy.svg}
  \caption{Confusion matrices and total test set accuracies of different
           networks trained for 40 epochs each.}
  \label{fig:confusion_matrices}
\end{figure}

\noindent
Figure~\ref{fig:confusion_matrices} displays confusion matrices and total
test set accuracies for all four networks. The best performance (with an
accuracy of $0.367$) could be achieved by setting $\eta$ to $0.01$ and
$\lambda$ to 0.

\end{document}
