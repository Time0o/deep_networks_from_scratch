\documentclass{article}

\usepackage{dsfont}
\usepackage{float}
\usepackage{minted}
\usepackage{svg}
\usepackage{tcolorbox}

\usemintedstyle{xcode}

\BeforeBeginEnvironment{listing}{\begin{tcolorbox}}%
\AfterEndEnvironment{listing}{\end{tcolorbox}}%

\begin{document}

\title{DD2424 - Assignment 1 Bonus}
\author{Timo Nicolai}

\maketitle

\section{Improving Accuracy}

I went through the list of suggested improvements and skipped the ones that
would have been too computationally intensive for the hardware available to me
(foremost hyperparameter search which I suspect could have had a significant
impact on final classifier performance but would have taken several hours to
run for a reasonable number of parameter values and learning rate decay which
did not seem to have a large impact when applied during the first hundred or so
epochs and would likely have required longer training and a search over the
optimal learning rate schedule). I also didn't implement Xavier initialization
which is not all that different from random initialization with fixed standard
deviation for a single layer network.

I progressed in an incremental fashion, i.e. if a technique yielded performance
improvements I used it in combination with all further techniques.

Overall, adding more training data caused the largest jump in validation set
performance. Most other techniques only produced meager improvements (or none
at all), maybe with the exception of ensemble learning which boosted validation
set performance by about 1\%.

\subsection{More Training Data}
One would expect large performance gains when greatly increasing the amount of
training data and indeed, using all training samples (except 1000 for the
validation set) and training with the same training parameters as before
($\eta = 0.01$, batch size 100, 40 epochs), the network achieves a total
classification accuracy of around 41.4\% on the validation set.
Figure~\ref{fig:learning_curves_full} shows the corresponding learning curves
and Figure~\ref{fig:confusion_matrix_full} shows the validation set confusion
matrix (I only evaluated performance on the test set again once I had tested
all the difference improvements so as not to include or discard them based
on test set performance).

\subsection{Early Stopping}
I implemented early stopping as follows: I stopped the training once the
validation cost did not improve over its previous best value for a certain
number of consecutive epochs (here I choose 10). The final network parameters
were then chosen as those that achieved the best validation set
\textbf{accuracy} at the end of some epoch (accuracy is not exactly ``smooth''
across epochs and thus it makes more sense to use the cost to determine when to
stop training).

Figure~\ref{fig:learning_curves_stop_early} shows learning curves for this
case, the training terminated after 91 epochs. The fact that the validation
error flatlines relatively early might suggest that fine-tuning of the learning
rate and the regularization parameter is in order but as mentioned before I
did not perform any sort of gridsearch in the interest of time.

The best performing network parameters yielded only a slight improvement of
around 0.5\% (see Figure~\ref{fig:confusion_matrix_stop_early}) which I suspect
might not correlate with increased test set performance. Because the best
accuracy occurred at around 20 epochs I did not stop training prematurely in all
further experiments but still always chose the intermediate network parameters.

\subsection{Data Augmentation}
To keep things simple I applied small random variations to brightness, contrast
and saturation to each of the images in the training set.
Figure~\ref{fig:data_unaugmented} shows some of the original images from the
training set and Figure~\ref{fig:data_augmented} shows the same images after
applying random augmentations (the results are somewhat subtle in some cases).

I did however not do this on a per batch basis (that would have been
prohibitively expensive). Instead I randomly transformed each image once and
added the result of this transformation to the training set, effectively
doubling the size of the training set. Unfortunately this did not improve final
validation accuracy at all, as show in
Figure~\ref{fig:confusion_matrix_augment}. I did not use data augmentation
while training the final network.

\subsection{Random Shuffling}
Random shuffling of the training data mainly resulted in a large jitter of
the learning curves but no perceptible improvement in their overall trends
as can be seen in Figure~\ref{fig:learning_curves_shuffle}. I still decided
to use shuffling because I suspected it might improve performance of an
ensemble classifier (see next section).

\subsection{Ensemble Learning}
For the final classifier I trained ten networks with different random parameter
initializations on different training sets obtained via bagging (sampling with
replacement from the original training set), I randomly shuffled the training
data at the beginning of each epoch and chose the best intermediate parameters
after 40 epochs of training for each weak learner.
Figure~\ref{fig:ensemble_performance_val} shows the performance of this
ensemble on the validation set which is 1\% higher than before. The final
performance this ensemble achieves on the test set is shown in
Figure~\ref{fig:ensemble_performance_test}. At around 40\% it is sadly a lot
lower than the validation set performance albeit still higher than the result
before optimization.

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth]{../figures/learning_curves_full.svg}
  \caption{Learning curves for full training set.}
  \label{fig:learning_curves_full}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=0.7\textwidth]{../figures/confusion_matrix_full.svg}
  \caption{Confusion matrix for full training set.}
  \label{fig:confusion_matrix_full}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth]{../figures/learning_curves_stop_early.svg}
  \caption{Learning curves for full training set with early stopping.}
  \label{fig:learning_curves_stop_early}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=0.7\textwidth]{../figures/confusion_matrix_stop_early.svg}
  \caption{Confusion matrix for full training set with early stopping.}
  \label{fig:confusion_matrix_stop_early}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=0.6\textwidth]{../figures/data_unaugmented.svg}
  \caption{Some unaugmented training samples.}
  \label{fig:data_unaugmented}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=0.6\textwidth]{../figures/data_augmented.svg}
  \caption{Training samples from Figure~\ref{fig:confusion_matrix_augment} after
           applying random augmentations.}
  \label{fig:data_augmented}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=0.7\textwidth]{../figures/confusion_matrix_augment.svg}
  \caption{Confusion matrix for a network trained with augmented training data.}
  \label{fig:confusion_matrix_augment}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=\textwidth]{../figures/learning_curves_shuffle.svg}
  \caption{Learning curves for full training with random shuffling between epochs.}
  \label{fig:learning_curves_shuffle}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=0.65\textwidth]{../figures/ensemble_performance_val.svg}
  \caption{Confusion matrix for an ensemble of 10 networks trained via bagging
           (validation set).}
  \label{fig:ensemble_performance_val}
\end{figure}

\begin{figure}[H]
  \centering
  \includesvg[width=0.65\textwidth]{../figures/ensemble_performance_test.svg}
  \caption{Confusion matrix for an ensemble of 10 networks trained via bagging
           (test set).}
  \label{fig:ensemble_performance_test}
\end{figure}

\pagebreak

\section{SVM Loss}

For this part I dropped the bias vector and instead appended an extra row of
ones to all input data matrices (and a corresponding extra column to the weight
matrix). This simplifies the gradient computations. My implementation follows
the following analytical formula for the weight matrix gradient:

$$
\frac{\partial l_{SVM, i}}{\partial w_j} =
\left\{
    \begin{array}{ll}
        -\left(
          \sum_{j \neq y_i} \mathds{1}(w_j^T x_i - w_{y_i}^T x_i + 1 > 0)
        \right)
        \cdot x_i & \mbox{if } j = y_i \\

        \mathds{1}(w_j^T x_i - w_{y_i}^T x_i + 1 > 0)
        \cdot x_i & \mbox{if } j \ne y_i
    \end{array}
\right.
$$

\noindent
I was able to implement this in an efficient vectorized form on a per-batch
basis

\bigskip

\begin{figure}
  \centering
    \begin{tabular}{|c|c|c|c|}
    \hline
    data dimensions & batch size & $\lambda$ & $\epsilon_{rel,max}$ \\
    \hline
    20              & 1          & 0         & 3.66e-9              \\
    \hline
    20              & 1          & 0.5       & 1.57e-6              \\
    \hline
    20              & 20         & 0         & 4.56-7               \\
    \hline
    20              & 20         & 0.5       & 4.35e-4              \\
    \hline
    \end{tabular}
  \caption{Maximum relative SVM gradient error between analytical and numerical
           gradient. Maximum taken over all elements of a gradient matrix
           resulting from the same random weight matrix initialization. Numerical
           gradient obtained via simple forward difference with $h = 1e-6$.}
  \label{fig:svm_gradient}
\end{figure}

\noindent
I verified the correctness of my implementation as before by comparing
analytical and numerical gradients for different batch size and regularization
parameters. Figure~\ref{fig:svm_gradient} displays the maximum relative error
for several of these cases. These values are somewhat low across the board and so
I am certain that my implementation functions correctly.

\begin{figure}
  \centering
  \includesvg[width=0.7\textwidth]{../figures/learning_curves_svm.svg}
  \caption{Learning curves for networks training with SVM loss and different
           learning rates and regularization parameters.}
  \label{fig:svm_learning_curves}
\end{figure}

I trained four different networks with the SVM loss using similar setups as
during the initial training runs with the cross entropy loss. I had to lower
the learning rate to achieve good results, $\eta = 0.01$ already caused the
training to diverge as can be seen in Figure~\ref{fig:svm_learning_curves}.

\begin{figure}
  \centering
  \includesvg[width=1\textwidth]{../figures/confusion_matrices_svm.svg}
  \caption{Test set confusion matrices corresponding to the networks from
           Figure~\ref{fig:svm_learning_curves}.}
  \label{fig:svm_confusion_matrices}
\end{figure}

Figure~\ref{fig:svm_confusion_matrices} displays final classification accuracy
on the test set and confusion matrices for these four networks. Using the SVM
loss did not yield an improvement over the accuracy achieved with the cross
entropy loss for the respective best networks. At the same time, increased
regularization proved to be slightly less detrimental to classification
accuracy when the SVM loss was used.

Figure~\ref{fig:svm_total_accuracy} shows a more direct comparison of final
test set accuracies for all networks trained with sensible parameters.

\begin{figure}
  \centering
  \begin{tabular}{ll}
    \begin{tabular}{|c|c|c|}
      \multicolumn{3}{c}{\textbf{Cross Entropy Loss}} \\
      \hline
      $\eta$ & $\lambda$ & Accuracy \\
      \hline
      0.01   & 0         & 0.367    \\
      \hline
      0.01   & 0.1       & 0.334    \\
      \hline
      0.01   & 1         & 0.219    \\
      \hline
    \end{tabular}
    &
    \begin{tabular}{|c|c|c|}
      \multicolumn{3}{c}{\textbf{SVM loss}} \\
      \hline
      $\eta$ & $\lambda$ & Accuracy \\
      \hline
      0.001  & 0         & 0.360    \\
      \hline
      0.001  & 0.1       & 0.353    \\
      \hline
      0.001  & 1         & 0.332    \\
      \hline
    \end{tabular}
  \end{tabular}
  \caption{Comparison of achieved test set accuracy for networks trained with
           cross entropy and SVM loss.}
  \label{fig:svm_total_accuracy}
\end{figure}

\end{document}
